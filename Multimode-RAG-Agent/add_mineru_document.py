#!/usr/bin/env python3
"""
MinerUæ–‡æ¡£å¤„ç†è„šæœ¬
åŠŸèƒ½ï¼š
1. å°†MinerUè§£æçš„å›¾ç‰‡å¤åˆ¶åˆ°å‰ç«¯publicç›®å½•
2. ä¿®æ”¹MDæ–‡ä»¶ä¸­çš„å›¾ç‰‡è·¯å¾„å¼•ç”¨
3. å°†å¤„ç†åçš„MDå†…å®¹æ·»åŠ åˆ°å‘é‡åº“

Author: AI Assistant
Date: 2025-01-31
"""

import os
import shutil
from pathlib import Path
import re
from typing import List
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¯¼å…¥LangChainç›¸å…³åŒ…
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

# é…ç½®å‚æ•°
MINERU_DIR = "MinerU File"
FRONTEND_PUBLIC_DIR = "agent-chat-ui/public"
VECTORSTORE_PATH = "mcp_course_materials_db"
MD_FILE = "full.md"
IMAGES_DIR = "images"

# æ–°çš„å›¾ç‰‡è·¯å¾„å‰ç¼€ï¼ˆå‰ç«¯è®¿é—®è·¯å¾„ï¼‰
IMAGE_PREFIX = "/langchain-course-images"

def setup_directories():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
    frontend_images_dir = Path(FRONTEND_PUBLIC_DIR) / "langchain-course-images"
    frontend_images_dir.mkdir(parents=True, exist_ok=True)
    print(f"âœ… åˆ›å»ºå‰ç«¯å›¾ç‰‡ç›®å½•: {frontend_images_dir}")
    return frontend_images_dir

def copy_images_to_frontend(source_dir: Path, target_dir: Path) -> List[str]:
    """å¤åˆ¶å›¾ç‰‡åˆ°å‰ç«¯ç›®å½•"""
    copied_images = []
    source_images_dir = source_dir / IMAGES_DIR
    
    if not source_images_dir.exists():
        print(f"âš ï¸  å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {source_images_dir}")
        return copied_images
    
    for image_file in source_images_dir.glob("*.jpg"):
        target_file = target_dir / image_file.name
        shutil.copy2(image_file, target_file)
        copied_images.append(image_file.name)
        print(f"ğŸ“· å¤åˆ¶å›¾ç‰‡: {image_file.name}")
    
    print(f"âœ… æ€»å…±å¤åˆ¶äº† {len(copied_images)} å¼ å›¾ç‰‡")
    return copied_images

def update_image_paths_in_md(md_content: str) -> str:
    """æ›´æ–°MDæ–‡ä»¶ä¸­çš„å›¾ç‰‡è·¯å¾„"""
    # åŒ¹é…å›¾ç‰‡å¼•ç”¨æ ¼å¼: ![](images/filename.jpg)
    pattern = r'!\[\]\(images/([^)]+)\)'
    
    def replace_image_path(match):
        filename = match.group(1)
        new_path = f"{IMAGE_PREFIX}/{filename}"
        return f"![]({new_path})"
    
    updated_content = re.sub(pattern, replace_image_path, md_content)
    
    # ç»Ÿè®¡æ›¿æ¢çš„å›¾ç‰‡æ•°é‡
    original_count = len(re.findall(pattern, md_content))
    print(f"ğŸ”„ æ›´æ–°äº† {original_count} ä¸ªå›¾ç‰‡è·¯å¾„å¼•ç”¨")
    
    return updated_content

def split_markdown_content(content: str) -> List[Document]:
    """åˆ†å‰²markdownå†…å®¹ä¸ºæ–‡æ¡£å—"""
    # é…ç½®æ–‡æœ¬åˆ†å‰²å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=[
            "\n# ",      # ä¸€çº§æ ‡é¢˜
            "\n## ",     # äºŒçº§æ ‡é¢˜  
            "\n### ",    # ä¸‰çº§æ ‡é¢˜
            "\n\n",      # æ®µè½
            "\n",        # è¡Œ
            " ",         # è¯
            ""           # å­—ç¬¦
        ]
    )
    
    # åˆ†å‰²æ–‡æœ¬
    texts = text_splitter.split_text(content)
    
    # åˆ›å»ºDocumentå¯¹è±¡
    documents = []
    for i, text in enumerate(texts):
        doc = Document(
            page_content=text,
            metadata={
                "source": "LangChainè¯¾ç¨‹æ–‡æ¡£",
                "chunk_id": i,
                "document_type": "course_material",
                "processed_by": "MinerU"
            }
        )
        documents.append(doc)
    
    print(f"ğŸ“„ æ–‡æ¡£åˆ†å‰²ä¸º {len(documents)} ä¸ªå—")
    return documents

def add_to_vectorstore(documents: List[Document]):
    """å°†æ–‡æ¡£æ·»åŠ åˆ°å‘é‡åº“"""
    try:
        # åˆå§‹åŒ–embeddings
        embeddings = OpenAIEmbeddings(
            api_key=os.getenv("OPENAI_API_KEY", "your-openai-api-key-here"),
            base_url="https://ai.devtool.tech/proxy/v1",
            model="text-embedding-3-small",
        )
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç°æœ‰å‘é‡åº“
        vectorstore_path = Path(VECTORSTORE_PATH)
        
        if vectorstore_path.exists():
            print("ğŸ“š åŠ è½½ç°æœ‰å‘é‡åº“...")
            # åŠ è½½ç°æœ‰å‘é‡åº“
            vectorstore = FAISS.load_local(
                folder_path=VECTORSTORE_PATH,
                embeddings=embeddings,
                allow_dangerous_deserialization=True,
            )
            
            # æ·»åŠ æ–°æ–‡æ¡£
            vectorstore.add_documents(documents)
            print("âœ… æ–°æ–‡æ¡£å·²æ·»åŠ åˆ°ç°æœ‰å‘é‡åº“")
            
        else:
            print("ğŸ†• åˆ›å»ºæ–°çš„å‘é‡åº“...")
            # åˆ›å»ºæ–°çš„å‘é‡åº“
            vectorstore = FAISS.from_documents(documents, embeddings)
            print("âœ… æ–°å‘é‡åº“åˆ›å»ºæˆåŠŸ")
        
        # ä¿å­˜å‘é‡åº“
        vectorstore.save_local(VECTORSTORE_PATH)
        print(f"ğŸ’¾ å‘é‡åº“å·²ä¿å­˜åˆ°: {VECTORSTORE_PATH}")
        
        # æ˜¾ç¤ºå‘é‡åº“ä¿¡æ¯
        print(f"ğŸ“Š å‘é‡åº“æ€»æ–‡æ¡£æ•°: {vectorstore.index.ntotal}")
        
    except Exception as e:
        print(f"âŒ å‘é‡åº“æ“ä½œå¤±è´¥: {str(e)}")
        raise

def validate_environment():
    """éªŒè¯ç¯å¢ƒé…ç½®"""
    required_keys = ["OPENAI_API_KEY"]  # Please set your API key in .env file
    missing_keys = []
    
    for key in required_keys:
        if not os.getenv(key):
            missing_keys.append(key)
    
    if missing_keys:
        print(f"âŒ ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡: {', '.join(missing_keys)}")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®è¿™äº›å˜é‡")
        return False
    
    print("âœ… ç¯å¢ƒå˜é‡éªŒè¯é€šè¿‡")
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¤„ç†MinerUæ–‡æ¡£...")
    
    # éªŒè¯ç¯å¢ƒ
    if not validate_environment():
        return
    
    # æ£€æŸ¥æºæ–‡ä»¶
    source_dir = Path(MINERU_DIR)
    md_file_path = source_dir / MD_FILE
    
    if not md_file_path.exists():
        print(f"âŒ æ‰¾ä¸åˆ°MDæ–‡ä»¶: {md_file_path}")
        return
    
    try:
        # 1. è®¾ç½®ç›®å½•
        frontend_images_dir = setup_directories()
        
        # 2. å¤åˆ¶å›¾ç‰‡åˆ°å‰ç«¯
        copied_images = copy_images_to_frontend(source_dir, frontend_images_dir)
        
        # 3. è¯»å–å’Œå¤„ç†MDæ–‡ä»¶
        print(f"ğŸ“– è¯»å–MDæ–‡ä»¶: {md_file_path}")
        with open(md_file_path, 'r', encoding='utf-8') as f:
            md_content = f.read()
        
        # 4. æ›´æ–°å›¾ç‰‡è·¯å¾„
        updated_content = update_image_paths_in_md(md_content)
        
        # 5. ä¿å­˜æ›´æ–°åçš„MDæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        updated_md_path = source_dir / "full_updated.md"
        with open(updated_md_path, 'w', encoding='utf-8') as f:
            f.write(updated_content)
        print(f"ğŸ’¾ ä¿å­˜æ›´æ–°åçš„MDæ–‡ä»¶: {updated_md_path}")
        
        # 6. åˆ†å‰²å†…å®¹
        documents = split_markdown_content(updated_content)
        
        # 7. æ·»åŠ åˆ°å‘é‡åº“
        add_to_vectorstore(documents)
        
        print("ğŸ‰ å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“ æ€»ç»“:")
        print(f"   - å¤åˆ¶å›¾ç‰‡: {len(copied_images)} å¼ ")
        print(f"   - æ–‡æ¡£åˆ†å—: {len(documents)} ä¸ª")
        print(f"   - å‘é‡åº“è·¯å¾„: {VECTORSTORE_PATH}")
        print(f"   - å‰ç«¯å›¾ç‰‡è·¯å¾„: {frontend_images_dir}")
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()